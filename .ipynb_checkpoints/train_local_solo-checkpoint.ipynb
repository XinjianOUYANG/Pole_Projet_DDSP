{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import ddsp.training\n",
    "from matplotlib import pyplot as plt\n",
    "import IPython.display as ipd\n",
    "import gin\n",
    "import numpy as np\n",
    "import utils\n",
    "import tensorflow as tf\n",
    "\n",
    "%matplotlib inline\n",
    "sample_rate = 16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "form",
    "id": "A0bK6P9DMBTb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive Folder Exists: Pretrained_ Models_for_T2/training\n"
     ]
    }
   ],
   "source": [
    "DRIVE_DIR = 'Pretrained_ Models_for_T2/training' \n",
    "print('Drive Folder Exists:', DRIVE_DIR)\n",
    "assert os.path.exists(DRIVE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FELlizMtIxCH"
   },
   "source": [
    "## Make directories to save model and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Qd22WxEQI3FV"
   },
   "outputs": [],
   "source": [
    "AUDIO_DIR = 'Pretrained_ Models_for_T2/training/Audio'\n",
    "AUDIO_FILEPATTERN = AUDIO_DIR + '/*'\n",
    "!mkdir -p $AUDIO_DIR\n",
    "\n",
    "SAVE_DIR = os.path.join(DRIVE_DIR, 'ddsp-solo-instrument')\n",
    "!mkdir -p \"$SAVE_DIR\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fb4YD8woYD1H"
   },
   "source": [
    "## Prepare Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uNhH7nEbX2db"
   },
   "source": [
    "#### Upload training audio\n",
    "\n",
    "Upload audio files to use for training your model. Uses `DRIVE_DIR` if connected to drive, otherwise prompts local upload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "itVKEzF6m3rY"
   },
   "outputs": [],
   "source": [
    "mp3_files = glob.glob(os.path.join(DRIVE_DIR, '*.mp3'))\n",
    "wav_files = glob.glob(os.path.join(DRIVE_DIR, '*.wav'))\n",
    "audio_files = mp3_files + wav_files\n",
    "\n",
    "for fname in audio_files:\n",
    "  target_name = os.path.join(AUDIO_DIR, \n",
    "                             os.path.basename(fname).replace(' ', '_'))\n",
    "  print('Copying {} to {}'.format(fname, target_name))\n",
    "  !cp \"$fname\" $target_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g_XVFoN2YOat"
   },
   "source": [
    "### Preprocess raw audio into TFRecord dataset\n",
    "\n",
    "We need to do some preprocessing on the raw audio you uploaded to get it into the correct format for training. This involves turning the full audio into short (4-second) examples, inferring the fundamental frequency (or \"pitch\") with [CREPE](http://github.com/marl/crepe), and computing the loudness. These features will then be stored in a sharded [TFRecord](https://www.tensorflow.org/tutorials/load_data/tfrecord) file for easier loading. Depending on the amount of input audio, this process usually takes a few minutes.\n",
    "\n",
    "* (Optional) Transfer dataset from drive. If you've already created a dataset, from a previous run, this cell will skip the dataset creation step and copy the dataset from `$DRIVE_DIR/data` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "MsnkAHyHVrCW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: Models_for_T2/training/Audio/*\n",
      "Saving to Pretrained_ Models_for_T2/training/data\n",
      "zsh:1: no matches found: data/train.tfrecord*\n"
     ]
    }
   ],
   "source": [
    "TRAIN_TFRECORD = 'data/train.tfrecord'\n",
    "TRAIN_TFRECORD_FILEPATTERN = TRAIN_TFRECORD + '*'\n",
    "\n",
    "# Copy dataset from drive if dataset has already been created.\n",
    "drive_data_dir = os.path.join(DRIVE_DIR, 'data') \n",
    "drive_dataset_files = glob.glob(drive_data_dir + '/*')\n",
    "\n",
    "if DRIVE_DIR and len(drive_dataset_files) > 0:\n",
    "  !cp \"$drive_data_dir\"/* data/\n",
    "\n",
    "else:\n",
    "  # Make a new dataset.\n",
    "  if not glob.glob(AUDIO_FILEPATTERN):\n",
    "    raise ValueError('No audio files found. Please use the previous cell to '\n",
    "                    'upload.')\n",
    "\n",
    "  !ddsp_prepare_tfrecord \\\n",
    "    --input_audio_filepatterns=$AUDIO_FILEPATTERN \\\n",
    "    --output_tfrecord_path=$TRAIN_TFRECORD \\\n",
    "    --num_shards=10 \\\n",
    "    --alsologtostderr\n",
    "\n",
    "  # Copy dataset to drive for safe-keeping.\n",
    "  if DRIVE_DIR:\n",
    "    !mkdir \"$drive_data_dir\"/\n",
    "    print('Saving to {}'.format(drive_data_dir))\n",
    "    !cp $TRAIN_TFRECORD_FILEPATTERN \"$drive_data_dir\"/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d4toX-D-AYZL"
   },
   "source": [
    "### Save dataset statistics for timbre transfer\n",
    "\n",
    "Quantile normalization helps match loudness of timbre transfer inputs to the \n",
    "loudness of the dataset, so let's calculate it here and save in a pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Bp_c8P0xApY6"
   },
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Expected 'tf.Tensor(False, shape=(), dtype=bool)' to be true. Summarized data: b'No files matched pattern: data/train.tfrecord*'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-e9dc5610e935>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdata_provider\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mddsp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFRecordProvider\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAIN_TFRECORD_FILEPATTERN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_provider\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mPICKLE_FILE_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSAVE_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dataset_statistics.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_dataset_statistics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_provider\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPICKLE_FILE_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/DDSP/lib/python3.8/site-packages/ddsp/training/data.py\u001b[0m in \u001b[0;36mget_dataset\u001b[0;34m(self, shuffle)\u001b[0m\n\u001b[1;32m    199\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_single_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m     \u001b[0mfilenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file_pattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     dataset = filenames.interleave(\n\u001b[1;32m    203\u001b[0m         \u001b[0mmap_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_format_map_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/DDSP/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mlist_files\u001b[0;34m(file_pattern, shuffle, seed)\u001b[0m\n\u001b[1;32m   1222\u001b[0m           string_ops.reduce_join(file_pattern, separator=\", \"), name=\"message\")\n\u001b[1;32m   1223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1224\u001b[0;31m       assert_not_empty = control_flow_ops.Assert(\n\u001b[0m\u001b[1;32m   1225\u001b[0m           condition, [message], summarize=1, name=\"assert_not_empty\")\n\u001b[1;32m   1226\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0massert_not_empty\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/DDSP/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/DDSP/lib/python3.8/site-packages/tensorflow/python/util/tf_should_use.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[0;34m\"\"\"Decorates the input function.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m       return _add_should_use_warning(fn(*args, **kwargs),\n\u001b[0m\u001b[1;32m    248\u001b[0m                                      \u001b[0mwarn_in_eager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwarn_in_eager\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m                                      error_in_function=error_in_function)\n",
      "\u001b[0;32m~/opt/anaconda3/envs/DDSP/lib/python3.8/site-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mAssert\u001b[0;34m(condition, data, summarize, name)\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_n_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m       \u001b[0mdata_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_summarize_eager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummarize\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m       raise errors.InvalidArgumentError(\n\u001b[0m\u001b[1;32m    155\u001b[0m           \u001b[0mnode_def\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m           \u001b[0mop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Expected 'tf.Tensor(False, shape=(), dtype=bool)' to be true. Summarized data: b'No files matched pattern: data/train.tfrecord*'"
     ]
    }
   ],
   "source": [
    "data_provider = ddsp.training.data.TFRecordProvider(TRAIN_TFRECORD_FILEPATTERN)\n",
    "dataset = data_provider.get_dataset(shuffle=False)\n",
    "PICKLE_FILE_PATH = os.path.join(SAVE_DIR, 'dataset_statistics.pkl')\n",
    "\n",
    "utils.save_dataset_statistics(data_provider, PICKLE_FILE_PATH, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nIsq0HrzbOF7"
   },
   "source": [
    "Let's load the dataset in the `ddsp` library and have a look at one of the examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dA-FOmRgYdpZ"
   },
   "outputs": [],
   "source": [
    "data_provider = ddsp.training.data.TFRecordProvider(TRAIN_TFRECORD_FILEPATTERN)\n",
    "dataset = data_provider.get_dataset(shuffle=False)\n",
    "\n",
    "try:\n",
    "  ex = next(iter(dataset))\n",
    "except StopIteration:\n",
    "  raise ValueError(\n",
    "      'TFRecord contains no examples. Please try re-running the pipeline with '\n",
    "      'different audio file(s).')\n",
    "\n",
    "utils.specplot(ex['audio'])\n",
    "\n",
    "f, ax = plt.subplots(3, 1, figsize=(14, 4))\n",
    "x = np.linspace(0, 4.0, 1000)\n",
    "ax[0].set_ylabel('loudness_db')\n",
    "ax[0].plot(x, ex['loudness_db'])\n",
    "ax[1].set_ylabel('F0_Hz')\n",
    "ax[1].set_xlabel('seconds')\n",
    "ax[1].plot(x, ex['f0_hz'])\n",
    "ax[2].set_ylabel('F0_confidence')\n",
    "ax[2].set_xlabel('seconds')\n",
    "ax[2].plot(x, ex['f0_confidence'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(ex['audio'], rate = sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9gvXBa7PbuyY"
   },
   "source": [
    "## Train Model\n",
    "\n",
    "We will now train a \"solo instrument\" model. This means the model is conditioned only on the fundamental frequency (f0) and loudness with no instrument ID or latent timbre feature. If you uploaded audio of multiple instruemnts, the neural network you train will attempt to model all timbres, but will likely associate certain timbres with different f0 and loudness conditions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YpwQkSIKjEMZ"
   },
   "source": [
    "First, let's start up a [TensorBoard](https://www.tensorflow.org/tensorboard) to monitor our loss as training proceeds. \n",
    "\n",
    "Initially, TensorBoard will report `No dashboards are active for the current data set.`, but once training begins, the dashboards should appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u2lx7yJneUXT"
   },
   "outputs": [],
   "source": [
    "%reload_ext tensorboard\n",
    "import tensorboard as tb\n",
    "tb.notebook.start('--logdir \"{}\"'.format(SAVE_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fT-8Koyvj46w"
   },
   "source": [
    "### We will now begin training. \n",
    "\n",
    "Note that we specify [gin configuration](https://github.com/google/gin-config) files for the both the model architecture ([solo_instrument.gin](TODO)) and the dataset ([tfrecord.gin](TODO)), which are both predefined in the library. You could also create your own. We then override some of the spefic params for `batch_size` (which is defined in in the model gin file) and the tfrecord path (which is defined in the dataset file). \n",
    "\n",
    "### Training Notes:\n",
    "* Models typically perform well when the loss drops to the range of ~4.5-5.0.\n",
    "* Depending on the dataset this can take anywhere from 5k-30k training steps usually.\n",
    "* The default is set to 30k, but you can stop training at any time, and for timbre transfer, it's best to stop before the loss drops too far below ~5.0 to avoid overfitting.\n",
    "* On the colab GPU, this can take from around 3-20 hours. \n",
    "* We **highly recommend** saving checkpoints directly to your drive account as colab will restart naturally after about 12 hours and you may lose all of your checkpoints.\n",
    "* By default, checkpoints will be saved every 300 steps with a maximum of 10 checkpoints (at ~60MB/checkpoint this is ~600MB). Feel free to adjust these numbers depending on the frequency of saves you would like and space on your drive.\n",
    "* If you're restarting a session and `DRIVE_DIR` points a directory that was previously used for training, training should resume at the last checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "poKO-mZEGYXZ"
   },
   "outputs": [],
   "source": [
    "!ddsp_run \\\n",
    "  --mode=train \\\n",
    "  --alsologtostderr \\\n",
    "  --save_dir=\"$SAVE_DIR\" \\\n",
    "  --gin_file=models/solo_instrument.gin \\\n",
    "  --gin_file=datasets/tfrecord.gin \\\n",
    "  --gin_param=\"TFRecordProvider.file_pattern='$TRAIN_TFRECORD_FILEPATTERN'\" \\\n",
    "  --gin_param=\"batch_size=16\" \\\n",
    "  --gin_param=\"train_util.train.num_steps=30000\" \\\n",
    "  --gin_param=\"train_util.train.steps_per_save=300\" \\\n",
    "  --gin_param=\"trainers.Trainer.checkpoints_to_keep=10\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V95qxVjFzWR6"
   },
   "source": [
    "## Resynthesis\n",
    "\n",
    "Check how well the model reconstructs the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OQ5PPDZVzgFR"
   },
   "outputs": [],
   "source": [
    "data_provider = ddsp.training.data.TFRecordProvider(TRAIN_TFRECORD_FILEPATTERN)\n",
    "dataset = data_provider.get_batch(batch_size=1, shuffle=False)\n",
    "\n",
    "try:\n",
    "  batch = next(iter(dataset))\n",
    "except OutOfRangeError:\n",
    "  raise ValueError(\n",
    "      'TFRecord contains no examples. Please try re-running the pipeline with '\n",
    "      'different audio file(s).')\n",
    "\n",
    "# Parse the gin config.\n",
    "gin_file = os.path.join(SAVE_DIR, 'operative_config-0.gin')\n",
    "gin.parse_config_file(gin_file)\n",
    "\n",
    "# Load model\n",
    "model = ddsp.training.models.Autoencoder()\n",
    "model.restore(SAVE_DIR)\n",
    "\n",
    "# Resynthesize audio.\n",
    "outputs = model(batch, training=False)\n",
    "audio_gen = model.get_audio_from_outputs(outputs)\n",
    "audio = batch['audio']\n",
    "\n",
    "print('Original Audio')\n",
    "specplot(audio)\n",
    "\n",
    "print('Resynthesis')\n",
    "specplot(audio_gen)\n",
    "play(audio_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Original Audio')\n",
    "ipd.Audio(audio[0], rate = sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Resynthesis')\n",
    "ipd.Audio(audio_gen[0], rate = sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZXM2ynLQ2Wl3"
   },
   "source": [
    "## Download Checkpoint\n",
    "\n",
    "Below you can download the final checkpoint. You are now ready to use it in the [DDSP Timbre Tranfer Colab](https://colab.research.google.com/github/magenta/ddsp/blob/master/ddsp/colab/demos/timbre_transfer.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2WDiCyXP0tNE"
   },
   "outputs": [],
   "source": [
    "CHECKPOINT_ZIP = 'my_solo_instrument.zip'\n",
    "latest_checkpoint_fname = os.path.basename(tf.train.latest_checkpoint(SAVE_DIR))\n",
    "!cd \"$SAVE_DIR\" && zip $CHECKPOINT_ZIP $latest_checkpoint_fname* operative_config-0.gin dataset_statistics.pkl\n",
    "!cp \"$SAVE_DIR/$CHECKPOINT_ZIP\" ./ #copy\n",
    "# colab_utils.download(CHECKPOINT_ZIP)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "hMqWDc_m6rUC"
   ],
   "name": "train_autoencoder.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
