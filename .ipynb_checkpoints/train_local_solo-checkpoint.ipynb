{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import ddsp.training\n",
    "from matplotlib import pyplot as plt\n",
    "import IPython.display as ipd\n",
    "import gin\n",
    "import numpy as np\n",
    "import utils\n",
    "import tensorflow as tf\n",
    "\n",
    "%matplotlib inline\n",
    "sample_rate = 16000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This DRIVE_DIR must includes your audio files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "DRIVE_DIR = './Pretrained_ Models_for_T2/training'  \n",
    "assert os.path.exists(DRIVE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "id": "Qd22WxEQI3FV"
   },
   "outputs": [],
   "source": [
    "# create all directories leading up to the given directory that do not exist already. \n",
    "# If the given directory already exists, ignore the error.\n",
    "DATA_DIR = os.path.join(DRIVE_DIR, 'data')\n",
    "!mkdir -p \"$DATA_DIR\"\n",
    "AUDIO_DIR = os.path.join(DATA_DIR, 'audio')\n",
    "AUDIO_FILEPATTERN = AUDIO_DIR + '/*'\n",
    "!mkdir -p \"$AUDIO_DIR\"\n",
    "# folder to save the model\n",
    "SAVE_DIR = os.path.join(DRIVE_DIR, 'ddsp-solo-instrument')\n",
    "!mkdir -p \"$SAVE_DIR\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fb4YD8woYD1H"
   },
   "source": [
    "## Prepare Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "id": "itVKEzF6m3rY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying ./Pretrained_ Models_for_T2/training/sax_1.wav to ./Pretrained_ Models_for_T2/training/data/audio/sax_1.wav\n"
     ]
    }
   ],
   "source": [
    "mp3_files = glob.glob(os.path.join(DRIVE_DIR, '*.mp3'))\n",
    "wav_files = glob.glob(os.path.join(DRIVE_DIR, '*.wav'))\n",
    "audio_files = mp3_files + wav_files\n",
    "\n",
    "for fname in audio_files:\n",
    "  target_name = os.path.join(AUDIO_DIR, \n",
    "                             os.path.basename(fname).replace(' ', '_'))\n",
    "  print('Copying {} to {}'.format(fname, target_name))\n",
    "  !cp \"$fname\" \"$target_name\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:root:Argument whitelist is deprecated. Please use allowlist.\n",
      "I0321 13:24:40.506776 4575448576 translations.py:641] ==================== <function annotate_downstream_side_inputs at 0x7fd0b40e6dc0> ====================\n",
      "I0321 13:24:40.507302 4575448576 translations.py:641] ==================== <function fix_side_input_pcoll_coders at 0x7fd0b40e6ee0> ====================\n",
      "I0321 13:24:40.507780 4575448576 translations.py:641] ==================== <function lift_combiners at 0x7fd0b40e81f0> ====================\n",
      "I0321 13:24:40.508028 4575448576 translations.py:641] ==================== <function expand_sdf at 0x7fd0b40e83a0> ====================\n",
      "I0321 13:24:40.508307 4575448576 translations.py:641] ==================== <function expand_gbk at 0x7fd0b40e8430> ====================\n",
      "I0321 13:24:40.508681 4575448576 translations.py:641] ==================== <function sink_flattens at 0x7fd0b40e8550> ====================\n",
      "I0321 13:24:40.508837 4575448576 translations.py:641] ==================== <function greedily_fuse at 0x7fd0b40e85e0> ====================\n",
      "I0321 13:24:40.510404 4575448576 translations.py:641] ==================== <function read_to_impulse at 0x7fd0b40e8670> ====================\n",
      "I0321 13:24:40.510516 4575448576 translations.py:641] ==================== <function impulse_to_input at 0x7fd0b40e8700> ====================\n",
      "I0321 13:24:40.510644 4575448576 translations.py:641] ==================== <function sort_stages at 0x7fd0b40e8940> ====================\n",
      "I0321 13:24:40.510914 4575448576 translations.py:641] ==================== <function setup_timer_mapping at 0x7fd0b40e88b0> ====================\n",
      "I0321 13:24:40.511141 4575448576 translations.py:641] ==================== <function populate_data_channel_coders at 0x7fd0b40e89d0> ====================\n",
      "I0321 13:24:40.513468 4575448576 statecache.py:174] Creating state cache with size 100\n",
      "I0321 13:24:40.513897 4575448576 worker_handlers.py:888] Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.EmbeddedWorkerHandler object at 0x7fd0b4337760> for environment ref_Environment_default_environment_1 (beam:env:embedded_python:v1, b'')\n",
      "I0321 13:24:40.514048 4575448576 fn_runner.py:510] Running (((((((((ref_AppliedPTransform_Create/Impulse_3)+(ref_AppliedPTransform_Create/FlatMap(<lambda at core.py:2957>)_4))+(ref_AppliedPTransform_Create/Map(decode)_6))+(ref_AppliedPTransform_Map(_load_audio)_7))+(ref_AppliedPTransform_Map(_add_f0_estimate)_8))+(ref_AppliedPTransform_Map(add_loudness)_9))+(ref_AppliedPTransform_FlatMap(split_example)_10))+(ref_AppliedPTransform_Reshuffle/AddRandomKeys_12))+(ref_AppliedPTransform_Reshuffle/ReshufflePerKey/Map(reify_timestamps)_14))+(Reshuffle/ReshufflePerKey/GroupByKey/Write)\n",
      "I0321 13:24:40.538312 4575448576 prepare_tfrecord_lib.py:59] Loading './Pretrained_ Models_for_T2/training/data/audio/sax_1.wav'.\n",
      "2021-03-21 13:24:40.685776: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-03-21 13:24:40.686002: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-03-21 13:24:41.254864: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n"
     ]
    }
   ],
   "source": [
    "TRAIN_DIR = os.path.join(DATA_DIR, 'train')\n",
    "!mkdir -p \"$TRAIN_DIR\"\n",
    "TRAIN_TFRECORD = TRAIN_DIR + '/train.tfrecord'\n",
    "\n",
    "!ddsp_prepare_tfrecord \\\n",
    "--input_audio_filepatterns=\"$AUDIO_FILEPATTERN \"\\\n",
    "--output_tfrecord_path=\"$TRAIN_TFRECORD\" \\\n",
    "--num_shards=10 \\\n",
    "--alsologtostderr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_TFRECORD_FILEPATTERN = TRAIN_DIR + '/train.tfrecord*'\n",
    "print(TRAIN_TFRECORD_FILEPATTERN )\n",
    "tfre_files = glob.glob(TRAIN_DIR + '/train.tfrecord*')\n",
    "print(tfre_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bp_c8P0xApY6"
   },
   "outputs": [],
   "source": [
    "data_provider = ddsp.training.data.TFRecordProvider(TRAIN_TFRECORD_FILEPATTERN)\n",
    "dataset = data_provider.get_dataset(shuffle=False)\n",
    "PICKLE_FILE_PATH = os.path.join(SAVE_DIR, 'dataset_statistics.pkl')\n",
    "\n",
    "utils.save_dataset_statistics(data_provider, PICKLE_FILE_PATH, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nIsq0HrzbOF7"
   },
   "source": [
    "Let's load the dataset in the `ddsp` library and have a look at one of the examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dA-FOmRgYdpZ"
   },
   "outputs": [],
   "source": [
    "data_provider = ddsp.training.data.TFRecordProvider(TRAIN_TFRECORD_FILEPATTERN)\n",
    "dataset = data_provider.get_dataset(shuffle=False)\n",
    "\n",
    "try:\n",
    "  ex = next(iter(dataset))\n",
    "except StopIteration:\n",
    "  raise ValueError(\n",
    "      'TFRecord contains no examples. Please try re-running the pipeline with '\n",
    "      'different audio file(s).')\n",
    "\n",
    "utils.specplot(ex['audio'])\n",
    "\n",
    "f, ax = plt.subplots(3, 1, figsize=(14, 4))\n",
    "x = np.linspace(0, 4.0, 1000)\n",
    "ax[0].set_ylabel('loudness_db')\n",
    "ax[0].plot(x, ex['loudness_db'])\n",
    "ax[1].set_ylabel('F0_Hz')\n",
    "ax[1].set_xlabel('seconds')\n",
    "ax[1].plot(x, ex['f0_hz'])\n",
    "ax[2].set_ylabel('F0_confidence')\n",
    "ax[2].set_xlabel('seconds')\n",
    "ax[2].plot(x, ex['f0_confidence'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(ex['audio'], rate = sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9gvXBa7PbuyY"
   },
   "source": [
    "## Train Model\n",
    "\n",
    "We will now train a \"solo instrument\" model. This means the model is conditioned only on the fundamental frequency (f0) and loudness with no instrument ID or latent timbre feature. If you uploaded audio of multiple instruemnts, the neural network you train will attempt to model all timbres, but will likely associate certain timbres with different f0 and loudness conditions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u2lx7yJneUXT"
   },
   "outputs": [],
   "source": [
    "%reload_ext tensorboard\n",
    "import tensorboard as tb\n",
    "tb.notebook.start('--logdir \"{}\"'.format(SAVE_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fT-8Koyvj46w"
   },
   "source": [
    "### We will now begin training. \n",
    "\n",
    "Note that we specify [gin configuration](https://github.com/google/gin-config) files for the both the model architecture ([solo_instrument.gin](TODO)) and the dataset ([tfrecord.gin](TODO)), which are both predefined in the library. You could also create your own. We then override some of the spefic params for `batch_size` (which is defined in in the model gin file) and the tfrecord path (which is defined in the dataset file). \n",
    "\n",
    "### Training Notes:\n",
    "* Models typically perform well when the loss drops to the range of ~4.5-5.0.\n",
    "* Depending on the dataset this can take anywhere from 5k-30k training steps usually.\n",
    "* The default is set to 30k, but you can stop training at any time, and for timbre transfer, it's best to stop before the loss drops too far below ~5.0 to avoid overfitting.\n",
    "* On the colab GPU, this can take from around 3-20 hours. \n",
    "* We **highly recommend** saving checkpoints directly to your drive account as colab will restart naturally after about 12 hours and you may lose all of your checkpoints.\n",
    "* By default, checkpoints will be saved every 300 steps with a maximum of 10 checkpoints (at ~60MB/checkpoint this is ~600MB). Feel free to adjust these numbers depending on the frequency of saves you would like and space on your drive.\n",
    "* If you're restarting a session and `DRIVE_DIR` points a directory that was previously used for training, training should resume at the last checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "poKO-mZEGYXZ"
   },
   "outputs": [],
   "source": [
    "!ddsp_run \\\n",
    "  --mode=train \\\n",
    "  --alsologtostderr \\\n",
    "  --save_dir=\"$SAVE_DIR\" \\\n",
    "  --gin_file=models/solo_instrument.gin \\\n",
    "  --gin_file=datasets/tfrecord.gin \\\n",
    "  --gin_param=\"TFRecordProvider.file_pattern='$TRAIN_TFRECORD_FILEPATTERN'\"\\\n",
    "  --gin_param=\"batch_size=16\" \\\n",
    "  --gin_param=\"train_util.train.num_steps=30000\" \\\n",
    "  --gin_param=\"train_util.train.steps_per_save=300\" \\\n",
    "  --gin_param=\"trainers.Trainer.checkpoints_to_keep=10\"\n",
    "\n",
    "# TFRecordProvider: https://github.com/magenta/ddsp/blob/master/ddsp/training/data.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V95qxVjFzWR6"
   },
   "source": [
    "## Resynthesis\n",
    "\n",
    "Check how well the model reconstructs the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OQ5PPDZVzgFR"
   },
   "outputs": [],
   "source": [
    "data_provider = ddsp.training.data.TFRecordProvider(TRAIN_TFRECORD_FILEPATTERN)\n",
    "dataset = data_provider.get_batch(batch_size=1, shuffle=False)\n",
    "\n",
    "try:\n",
    "  batch = next(iter(dataset))\n",
    "except OutOfRangeError:\n",
    "  raise ValueError(\n",
    "      'TFRecord contains no examples. Please try re-running the pipeline with '\n",
    "      'different audio file(s).')\n",
    "\n",
    "# Parse the gin config.\n",
    "gin_file = os.path.join(SAVE_DIR, 'operative_config-0.gin')\n",
    "gin.parse_config_file(gin_file)\n",
    "\n",
    "# Load model\n",
    "model = ddsp.training.models.Autoencoder()\n",
    "model.restore(SAVE_DIR)\n",
    "\n",
    "# Resynthesize audio.\n",
    "outputs = model(batch, training=False)\n",
    "audio_gen = model.get_audio_from_outputs(outputs)\n",
    "audio = batch['audio']\n",
    "\n",
    "print('Original Audio')\n",
    "specplot(audio)\n",
    "\n",
    "print('Resynthesis')\n",
    "specplot(audio_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Original Audio')\n",
    "ipd.Audio(audio[0], rate = sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Resynthesis')\n",
    "ipd.Audio(audio_gen[0], rate = sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZXM2ynLQ2Wl3"
   },
   "source": [
    "## Download Checkpoint\n",
    "\n",
    "Below you can download the final checkpoint. You are now ready to use it in the [DDSP Timbre Tranfer Colab](https://colab.research.google.com/github/magenta/ddsp/blob/master/ddsp/colab/demos/timbre_transfer.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2WDiCyXP0tNE"
   },
   "outputs": [],
   "source": [
    "CHECKPOINT_ZIP = 'my_solo_instrument.zip'\n",
    "latest_checkpoint_fname = os.path.basename(tf.train.latest_checkpoint(SAVE_DIR))\n",
    "!cd \"$SAVE_DIR\" && zip $CHECKPOINT_ZIP $latest_checkpoint_fname* operative_config-0.gin dataset_statistics.pkl\n",
    "!cp \"$SAVE_DIR/$CHECKPOINT_ZIP\" ./ #copy\n",
    "# colab_utils.download(CHECKPOINT_ZIP)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "hMqWDc_m6rUC"
   ],
   "name": "train_autoencoder.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
