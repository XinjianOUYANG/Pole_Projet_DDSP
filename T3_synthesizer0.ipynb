{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JXctfLGdacKx"
   },
   "source": [
    "## [Our DDSP project in Github](https://github.com/XinjianOUYANG/Pole_Projet_DDSP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDSP synthesizer\n",
    "\n",
    "Evaluate the generation quality of the DDSP synthesizer using the GMM from T3.2 and predefined pitch and loudness profiles.\n",
    "\n",
    "####  Expected outcome of T3: \n",
    "A fully generative model based on DDSP which can generate musical sounds without any input audio signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TUUGDMY5qSdz"
   },
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m2Ol13FaZKwh"
   },
   "outputs": [],
   "source": [
    "# Ignore a bunch of deprecation warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import tensorflow.compat.v2 as tf\n",
    "import tensorflow_datasets as tfds \n",
    "import ddsp\n",
    "import utils\n",
    "import os\n",
    "import gin\n",
    "import pickle\n",
    "import matplotlib\n",
    "import IPython.display as ipd\n",
    "import numpy as np\n",
    "import soundfile as sf \n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "import librosa\n",
    "import librosa.display\n",
    " \n",
    "%matplotlib inline\n",
    "sample_rate = 16000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the path of audio and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# audio path\n",
    "audio_path = 'Datasets/Piano/Audio/Piano_01.wav'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**the model trained with z encoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model folder direction \n",
    "model_dir_z = 'Pretrained_Models_for_T2/piano_ae'\n",
    "# dataset_statistics.pkl in .model folder\n",
    "dataset_stats_file_z = os.path.join(model_dir_z, 'dataset_statistics.pkl')\n",
    "# operative_config-0.gin in model folder\n",
    "gin_file_z = os.path.join(model_dir_z, 'operative_config-0.gin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gzUgnAjhrP9g"
   },
   "source": [
    "## Read audios and computing (f0,loudness,MFCC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lObag5Jd9amV"
   },
   "source": [
    "### Use the given python file [spectral_ops.py](https://github.com/magenta/ddsp/blob/master/ddsp/spectral_ops.py) in ddsp library to compute $f0$ and loudness.\n",
    "* *ddsp.spectral_ops.compute_f0*\n",
    "\n",
    "* *ddsp.spectral_ops.compute_loudness*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "executionInfo": {
     "elapsed": 570616,
     "status": "ok",
     "timestamp": 1615370354792,
     "user": {
      "displayName": "Xinjian OUYANG",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgDmgUNu_vljWNL53AJFQvh-vkweabOtVRZoY3kWA=s64",
      "userId": "04586718377978961617"
     },
     "user_tz": -60
    },
    "id": "jE5MqZnlMNNa",
    "outputId": "fc206f4f-689b-4efc-a04b-79d8f7d1b7d5"
   },
   "outputs": [],
   "source": [
    "print(audio_path)\n",
    "x_all, sr = sf.read(audio_path) #data,samplerate\n",
    "print('shape of original signal:',np.shape(x_all),'\\n','original sample rate:',sr)\n",
    "sig = x_all[:] # choose the first channel of the original audio\n",
    " \n",
    "# resample (down sampling to 16kHz) and take the 10-20 seconds\n",
    "sig_re = librosa.resample(sig,sr,sample_rate)\n",
    "audio = sig_re[10*sample_rate:20*sample_rate]\n",
    "print('audio shape:',np.shape(audio))\n",
    "audio = audio[np.newaxis,:]\n",
    "\n",
    "# # plot wave form\n",
    "# T_all = audio.shape[1]\n",
    "# time = np.arange(T_all)/sample_rate\n",
    "# plt.figure(figsize=(9,3))\n",
    "# plt.plot(time, audio[0])\n",
    "# plt.xlabel('time (s)')\n",
    "# plt.ylabel('amplitude')\n",
    "\n",
    "# # Calculate Spectrogram and plot\n",
    "# utils.specplot(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# play audio\n",
    "ipd.Audio(audio[0], rate = sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 778
    },
    "executionInfo": {
     "elapsed": 17211,
     "status": "ok",
     "timestamp": 1615371220642,
     "user": {
      "displayName": "Xinjian OUYANG",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgDmgUNu_vljWNL53AJFQvh-vkweabOtVRZoY3kWA=s64",
      "userId": "04586718377978961617"
     },
     "user_tz": -60
    },
    "id": "0gOzm3pp7vQb",
    "outputId": "928aa139-c5d3-4f66-843f-672db3051b51"
   },
   "outputs": [],
   "source": [
    "#extracting f0 with CREPE\n",
    "ddsp.spectral_ops.reset_crepe()\n",
    "f0_crepe, f0_confidence = ddsp.spectral_ops.compute_f0(audio[0], \n",
    "                                                       sample_rate= sample_rate,\n",
    "                                                       frame_rate=100,\n",
    "                                                       viterbi=False)\n",
    "#extracting loudness \n",
    "loudness =ddsp.spectral_ops.compute_loudness(audio[0],\n",
    "                     sample_rate= sample_rate,\n",
    "                     frame_rate=250,\n",
    "                     n_fft=2048,\n",
    "                     ref_db=20.7,\n",
    "                     use_tf=False)\n",
    "\n",
    "# audio_features dictionary\n",
    "audio_features_key = ['audio','f0_hz','f0_confidence','loudness_db']\n",
    "audio_features = dict([(k,[]) for k in audio_features_key])\n",
    "audio_features['audio'] = audio\n",
    "audio_features['f0_hz'] = f0_crepe\n",
    "audio_features['f0_confidence'] = f0_confidence\n",
    "audio_features['loudness_db'] = loudness\n",
    "\n",
    "# # Plot Pitch/f0.\n",
    "# plt.figure(figsize=(6, 4))\n",
    "# f0_crepe_midi = ddsp.core.hz_to_midi(f0_crepe)\n",
    "# plt.plot(np.ravel(f0_crepe), label='crepe')\n",
    "# plt.ylabel('Pitch (MIDI)')\n",
    "# # Plot f0_confidence.\n",
    "# plt.figure(figsize=(6, 4))\n",
    "# plt.plot(np.ravel(f0_confidence), label='f0 confidence')\n",
    "# plt.ylabel('f0 confidence')\n",
    "# # Plot Loundness.\n",
    "# plt.figure(figsize=(6, 4))\n",
    "# plt.plot(np.ravel(loudness), label='loudness')\n",
    "# plt.ylabel('Loudness')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yQ_iCEY5s9Mc"
   },
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loading(model_dir,dataset_stats_file,gin_file,audio_features,audio):\n",
    "    # Load the dataset statistics.\n",
    "    print(f'Loading dataset statistics from {dataset_stats_file}')\n",
    "    try:\n",
    "      if tf.io.gfile.exists(dataset_stats_file):\n",
    "        with tf.io.gfile.GFile(dataset_stats_file, 'rb') as f:\n",
    "          DATASET_STATS = pickle.load(f)\n",
    "    except Exception as err:\n",
    "      print('Loading dataset statistics from pickle failed: {}.'.format(err))\n",
    "\n",
    "\n",
    "    # Parse gin config,\n",
    "    with gin.unlock_config():\n",
    "      gin.parse_config_file(gin_file, skip_unknown=True)\n",
    "\n",
    "    # Assumes only one checkpoint in the folder, 'ckpt-[iter]`.\n",
    "    ckpt_files = [f for f in tf.io.gfile.listdir(model_dir) if 'ckpt' in f]\n",
    "    ckpt_name = ckpt_files[0].split('.')[0]\n",
    "    ckpt = os.path.join(model_dir, ckpt_name)\n",
    "\n",
    "    # Ensure dimensions and sampling rates are equal\n",
    "    time_steps_train = gin.query_parameter('F0LoudnessPreprocessor.time_steps')\n",
    "    n_samples_train = gin.query_parameter('Harmonic.n_samples')\n",
    "    hop_size = int(n_samples_train / time_steps_train)\n",
    "\n",
    "    time_steps = int(audio.shape[1] / hop_size)\n",
    "    n_samples = time_steps * hop_size\n",
    "\n",
    "    # print(\"===Trained model===\")\n",
    "    # print(\"Time Steps\", time_steps_train)\n",
    "    # print(\"Samples\", n_samples_train)\n",
    "    # print(\"Hop Size\", hop_size)\n",
    "    # print(\"\\n===Resynthesis===\")\n",
    "    # print(\"Time Steps\", time_steps)\n",
    "    # print(\"Samples\", n_samples)\n",
    "    # print('')\n",
    "\n",
    "    gin_params = [\n",
    "        'Harmonic.n_samples = {}'.format(n_samples),\n",
    "        'FilteredNoise.n_samples = {}'.format(n_samples),\n",
    "        'F0LoudnessPreprocessor.time_steps = {}'.format(time_steps),\n",
    "        'oscillator_bank.use_angular_cumsum = True',  # Avoids cumsum accumulation errors.\n",
    "    ]\n",
    "\n",
    "    with gin.unlock_config():\n",
    "      gin.parse_config(gin_params)\n",
    "\n",
    "    # Trim all input vectors to correct lengths \n",
    "    for key in ['f0_hz', 'f0_confidence', 'loudness_db']:\n",
    "      audio_features[key] = audio_features[key][:time_steps]\n",
    "    audio_features['audio'] = audio_features['audio'][:n_samples]\n",
    "\n",
    "    # Set up the model just to predict audio given new conditioning\n",
    "    model = ddsp.training.models.Autoencoder()\n",
    "    model.restore(ckpt)\n",
    "    # Resynthesize audio.\n",
    "    outputs = model(audio_features, training=False) # Run the forward pass, add losses, and create a dictionary of outputs.\n",
    "    audio_gen = outputs['audio_synth']\n",
    "    \n",
    "    return audio_gen, outputs, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model with z encoder\n",
    "print(model_dir_z,'\\n',dataset_stats_file_z,'\\n',gin_file_z)\n",
    "audio_gen_z, outputs_z, model_z = model_loading(model_dir_z,dataset_stats_file_z,gin_file_z,audio_features,audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract feature z from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_feature = outputs_z['z']\n",
    "\n",
    "print(outputs_z.keys())\n",
    "print('\\n shape of z:',np.shape(z_feature),'\\n')\n",
    "print(np.shape(audio_features['f0_hz']), np.shape(outputs_z['f0_hz']))\n",
    "print(np.shape(audio_features['loudness_db']), np.shape(outputs_z['loudness_db']))\n",
    "\n",
    "# print(audio_features['f0_hz'],outputs_z['f0_hz'],audio_features['loudness_db'],outputs_z['loudness_db'][0])\n",
    "# print('\\n',outputs_z['inputs'].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthesize pitch, loudness and z features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References\n",
    "* [autoencoder.py](https://github.com/magenta/ddsp/blob/master/ddsp/training/models/autoencoder.py)\n",
    "* [Scale f0, ld](https://github.com/magenta/ddsp/blob/master/ddsp/training/preprocessing.py)\n",
    "* [processor_group](https://github.com/magenta/ddsp/blob/d9124662d4e836a7665039f98e7421b9dd9a4178/ddsp/processors.py#L81)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f0_scaled = outputs_z['f0_scaled']\n",
    "ld_scaled = outputs_z['ld_scaled']\n",
    "f0_hz = outputs_z['f0_hz']\n",
    "print('\\n', np.shape(f0_scaled),np.shape(ld_scaled),np.shape(f0_hz))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GMM over z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "load_path = 'z_datasets/z_piano_ae.npy'\n",
    "\n",
    "X = np.load(load_path)\n",
    "print(np.shape(X))\n",
    "\n",
    "# reduce dimensionality \n",
    "N, T_step, Z_dim = np.shape(X)[0], np.shape(X)[2], np.shape(X)[3]\n",
    "X_new = np.zeros(shape=(N,T_step, Z_dim))\n",
    "\n",
    "for i in np.arange(N):\n",
    "    X_new[i] = X[i,0]\n",
    "    \n",
    "print(np.shape(X_new))\n",
    "\n",
    "X_re = np.reshape(X_new,(N*T_step,16))\n",
    "print(np.shape(X_re))\n",
    "\n",
    "# fit Z and build a GMM model\n",
    "gm = GaussianMixture(n_components=10).fit(X_re)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = gm.weights_\n",
    "print(weights,'\\n',np.argmax(weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict Z\n",
    "Z_frames = 2500\n",
    "\n",
    "predictions = gm.sample(Z_frames)\n",
    "Z = np.zeros(shape=(1,Z_frames, 16))\n",
    "Z[0] = predictions[0]\n",
    "\n",
    "print(np.shape(Z))\n",
    "\n",
    "# only one value of z \n",
    "# fix one value, constant pitch and loudness\n",
    "# sample from only on of the GM. inverse transform sampling, move smoothly\n",
    "# Presentation: whole project + personal contributions in one slide at the end. 15 mins presentation and 5 mins questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthesizing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_keys = ['ld_scaled', 'f0_scaled', 'z','f0_hz']\n",
    "inputs = dict([(k,[]) for k in inputs_keys])\n",
    "\n",
    "inputs['ld_scaled'] = ld_scaled \n",
    "inputs['f0_scaled'] = f0_scaled \n",
    "inputs['f0_hz'] = f0_hz\n",
    "inputs['z'] = Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_keys=('ld_scaled', 'f0_scaled', 'z')\n",
    "# 'model_z' is the pretrained model with z encoder\n",
    "pg_out = model_z.decode(inputs,training = False) # pg = processor group\n",
    "audio_syn = pg_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(audio_syn, rate = sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. fix z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = gm.sample(1)\n",
    "Z_fixed = np.zeros(shape=(1,Z_frames, 16))\n",
    "Z_fixed[0] = predictions[0]\n",
    "\n",
    "# print(Z_fixed, '\\n',np.shape(Z_fixed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs1_keys = ['ld_scaled', 'f0_scaled', 'z','f0_hz']\n",
    "inputs1 = dict([(k,[]) for k in inputs1_keys])\n",
    "\n",
    "inputs1['ld_scaled'] = ld_scaled \n",
    "inputs1['f0_scaled'] = f0_scaled \n",
    "inputs1['f0_hz'] = f0_hz\n",
    "inputs1['z'] = Z_fixed\n",
    "pg_out1 = model_z.decode(inputs1,training = False) # pg = processor group\n",
    "audio_syn1 = pg_out1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(audio_syn1, rate = sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. fix f,l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 600\n",
    "ld_scaled_temp = ld_scaled[0][i]\n",
    "f0_scaled_temp = f0_scaled[0][i]\n",
    "f0_hz_temp = f0_hz[0][i]\n",
    "print(ld_scaled_temp,'\\n', f0_scaled_temp,'\\n', f0_hz_temp)\n",
    "\n",
    "ld_scaled2 = np.zeros(shape=(1,Z_frames, 1))\n",
    "f0_scaled2 = np.zeros(shape=(1,Z_frames, 1))\n",
    "f0_hz2 = np.zeros(shape=(1,Z_frames, 1))\n",
    "ld_scaled2[0] = ld_scaled_temp\n",
    "f0_scaled2[0] = f0_scaled_temp\n",
    "f0_hz2[0] = f0_hz_temp\n",
    "\n",
    "inputs2_keys = ['ld_scaled', 'f0_scaled', 'z','f0_hz']\n",
    "inputs2 = dict([(k,[]) for k in inputs2_keys])\n",
    "\n",
    "inputs2['z'] = Z\n",
    "inputs2['ld_scaled'] = ld_scaled2 \n",
    "inputs2['f0_scaled'] = f0_scaled2 \n",
    "inputs2['f0_hz'] = f0_hz2\n",
    "\n",
    "# print(inputs['ld_scaled'],'\\n',inputs['f0_scaled'],'\\n',inputs['f0_hz'])\n",
    "pg_out2 = model_z.decode(inputs2,training = False) # pg = processor group\n",
    "audio_syn2 = pg_out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(audio_syn2, rate = sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Inverse transform sampling\n",
    "\n",
    "consider only the diagonal of the covariance matrix, and apply 1D inverse transform sampling for each dimensions, using as variance the corresponding element in the diagonal of the covariance matrix\n",
    " \n",
    "    norm.ppf: Percent point function (inverse of cdf — percentiles).\n",
    "    The location (loc) keyword specifies the mean. \n",
    "    The scale (scale) keyword specifies the standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "# take the gaussian of max weight from GMM\n",
    "mu = gm.means_[9] \n",
    "cov = gm.covariances_[9]\n",
    "\n",
    "# print(np.shape(cov),'\\n',cov)\n",
    "n_grid = Z_frames\n",
    "uniform_samples = np.linspace(0.01,0.99,n_grid) # linearly spaced points between 0 and 1\n",
    "normal_samples = np.zeros(shape=(1,Z_frames,16))\n",
    "\n",
    "for i in np.arange(16):\n",
    "    # map through inverse CDF\n",
    "    normal_samples[0,:,i] = norm.ppf(uniform_samples, loc=mu[i], scale=np.sqrt(cov[i][i])) \n",
    "    \n",
    "print(np.shape(normal_samples),'\\n',normal_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs3_keys = ['ld_scaled', 'f0_scaled', 'z','f0_hz']\n",
    "inputs3 = dict([(k,[]) for k in inputs3_keys])\n",
    "\n",
    "inputs3['ld_scaled'] = ld_scaled \n",
    "inputs3['f0_scaled'] = f0_scaled \n",
    "inputs3['f0_hz'] = f0_hz\n",
    "inputs3['z'] = normal_samples\n",
    "\n",
    "pg_out3 = model_z.decode(inputs3,training = False) # pg = processor group\n",
    "audio_syn3 = pg_out3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(audio_syn3, rate = sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UaLOL7hZZHav"
   },
   "source": [
    "## Comparing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('GMM')\n",
    "ipd.Audio(audio_syn, rate = sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('fix z')\n",
    "ipd.Audio(audio_syn1, rate = sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"fix f0 and loundness\")\n",
    "ipd.Audio(audio_syn2, rate = sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('z inverse transform sampling')\n",
    "ipd.Audio(audio_syn3, rate = sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Original')\n",
    "ipd.Audio(audio, rate = sample_rate) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Resynthesis with z encoder')\n",
    "ipd.Audio(audio_gen_z, rate = sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 971474,
     "status": "aborted",
     "timestamp": 1615370755684,
     "user": {
      "displayName": "Xinjian OUYANG",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgDmgUNu_vljWNL53AJFQvh-vkweabOtVRZoY3kWA=s64",
      "userId": "04586718377978961617"
     },
     "user_tz": -60
    },
    "id": "O83PMi_HlmLi",
    "outputId": "88070a53-5728-40cc-8df0-c5a051a93c10"
   },
   "outputs": [],
   "source": [
    "# Plot\n",
    "utils.specplot(audio)\n",
    "plt.title(\"Original\")\n",
    "\n",
    "utils.specplot(audio_gen_z)\n",
    "_ = plt.title(\"Resynthesis with z encoder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save audios**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import soundfile as sf\n",
    "# audio_syn_out = audio_syn[0]\n",
    "# sf.write('audio_syn.wav',audio_syn_out, sample_rate)\n",
    "# print(np.shape(audio_syn_out))\n",
    "\n",
    "# audio_original_out = audio[0]\n",
    "# sf.write('audio_original.wav',audio_original_out, sample_rate)\n",
    "\n",
    "# audio_z_out = audio_gen_z[0]\n",
    "# sf.write('audio_z.wav',audio_z_out, sample_rate)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "showing_models_local.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
