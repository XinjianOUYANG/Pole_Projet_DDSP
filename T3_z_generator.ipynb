{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "broadband-cornell",
   "metadata": {},
   "source": [
    "# Generate z datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formal-theory",
   "metadata": {},
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "floating-excerpt",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Argument whitelist is deprecated. Please use allowlist.\n"
     ]
    }
   ],
   "source": [
    "# Ignore a bunch of deprecation warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import tensorflow.compat.v2 as tf\n",
    "import tensorflow_datasets as tfds \n",
    "import ddsp\n",
    "import utils\n",
    "import os\n",
    "import gin\n",
    "import pickle\n",
    "import matplotlib\n",
    "import IPython.display as ipd\n",
    "import numpy as np\n",
    "import soundfile as sf \n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "import librosa\n",
    "import librosa.display\n",
    " \n",
    "%matplotlib inline\n",
    "sample_rate = 16000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honey-requirement",
   "metadata": {},
   "source": [
    "## Setting the path of audios and model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improved-trinity",
   "metadata": {},
   "source": [
    "### Model path with z encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "artistic-administrator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained_Models_for_T2/piano_ae \n",
      " Pretrained_Models_for_T2/piano_ae/dataset_statistics.pkl \n",
      " Pretrained_Models_for_T2/piano_ae/operative_config-0.gin\n",
      "find checkpoint: Pretrained_Models_for_T2/piano_ae/ckpt-8400\n"
     ]
    }
   ],
   "source": [
    "# model folder direction \n",
    "model_dir_z = 'Pretrained_Models_for_T2/piano_ae'\n",
    "model_name = os.path.basename(model_dir_z)\n",
    "\n",
    "# dataset_statistics.pkl in .model folder\n",
    "dataset_stats_file_z = os.path.join(model_dir_z, 'dataset_statistics.pkl')\n",
    "\n",
    "# operative_config-0.gin in model folder\n",
    "gin_file_z = os.path.join(model_dir_z, 'operative_config-0.gin')\n",
    "\n",
    "# Assumes only one checkpoint in the folder, 'ckpt-[iter]`.\n",
    "ckpt_files = [f for f in tf.io.gfile.listdir(model_dir_z) if 'ckpt' in f]\n",
    "ckpt_name = ckpt_files[0].split('.')[0]\n",
    "ckpt = os.path.join(model_dir_z, ckpt_name)\n",
    "\n",
    "print(model_dir_z,'\\n',dataset_stats_file_z,'\\n',gin_file_z)\n",
    "print('find checkpoint:',ckpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satisfactory-disaster",
   "metadata": {},
   "source": [
    "### Audio directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "spread-poison",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Datasets/Piano/Audio/Piano_01.wav', 'Datasets/Piano/Audio/Piano_02.wav', 'Datasets/Piano/Audio/Piano_03.wav', 'Datasets/Piano/Audio/Piano_04.wav', 'Datasets/Piano/Audio/Piano_05.wav', 'Datasets/Piano/Audio/Piano_06.wav', 'Datasets/Piano/Audio/Piano_07.wav', 'Datasets/Piano/Audio/Piano_08.wav', 'Datasets/Piano/Audio/Piano_09.wav', 'Datasets/Piano/Audio/Piano_10.wav', 'Datasets/Piano/Audio/Piano_11.wav', 'Datasets/Piano/Audio/Piano_12.wav', 'Datasets/Piano/Audio/Piano_13.wav', 'Datasets/Piano/Audio/Piano_14.wav', 'Datasets/Piano/Audio/Piano_15.wav']\n",
      "\n",
      " number of audios: 15\n"
     ]
    }
   ],
   "source": [
    "# audio directory path\n",
    "audio_dir = 'Datasets/Piano/Audio'\n",
    "audio_folder = []\n",
    "\n",
    "files = os.listdir(audio_dir)\n",
    "for tmp in files:\n",
    "    if os.path.splitext(tmp)[1] == '.wav' or os.path.splitext(tmp)[1] == '.mp3': \n",
    "        audio_folder.append(os.path.join(audio_dir,tmp))\n",
    "\n",
    "audio_folder.sort()\n",
    "print(audio_folder)\n",
    "audio_num = len(audio_folder)\n",
    "print('\\n number of audios:', audio_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accomplished-prospect",
   "metadata": {},
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "confirmed-villa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset statistics from Pretrained_Models_for_T2/piano_ae/dataset_statistics.pkl\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset statistics.\n",
    "print(f'Loading dataset statistics from {dataset_stats_file_z}')\n",
    "try:\n",
    "  if tf.io.gfile.exists(dataset_stats_file_z):\n",
    "    with tf.io.gfile.GFile(dataset_stats_file_z, 'rb') as f:\n",
    "      DATASET_STATS_Z = pickle.load(f)\n",
    "except Exception as err:\n",
    "  print('Loading dataset statistics from pickle failed: {}.'.format(err),'\\n')\n",
    "\n",
    "# Parse gin config,\n",
    "with gin.unlock_config():\n",
    "  gin.parse_config_file(gin_file_z, skip_unknown=True)\n",
    "time_steps_train = gin.query_parameter('F0LoudnessPreprocessor.time_steps')\n",
    "n_samples_train = gin.query_parameter('Harmonic.n_samples')\n",
    "hop_size = int(n_samples_train / time_steps_train)\n",
    "\n",
    "# Set up the model just to predict audio given new conditioning\n",
    "model = ddsp.training.models.Autoencoder()\n",
    "model.restore(ckpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amazing-instrumentation",
   "metadata": {},
   "source": [
    "## Generate z datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "raised-blood",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing the audio 1 / 15 : Datasets/Piano/Audio/Piano_01.wav\n",
      "processing the audio 2 / 15 : Datasets/Piano/Audio/Piano_02.wav\n",
      "processing the audio 3 / 15 : Datasets/Piano/Audio/Piano_03.wav\n",
      "processing the audio 4 / 15 : Datasets/Piano/Audio/Piano_04.wav\n",
      "processing the audio 5 / 15 : Datasets/Piano/Audio/Piano_05.wav\n",
      "processing the audio 6 / 15 : Datasets/Piano/Audio/Piano_06.wav\n",
      "processing the audio 7 / 15 : Datasets/Piano/Audio/Piano_07.wav\n",
      "processing the audio 8 / 15 : Datasets/Piano/Audio/Piano_08.wav\n",
      "processing the audio 9 / 15 : Datasets/Piano/Audio/Piano_09.wav\n",
      "processing the audio 10 / 15 : Datasets/Piano/Audio/Piano_10.wav\n",
      "processing the audio 11 / 15 : Datasets/Piano/Audio/Piano_11.wav\n",
      "processing the audio 12 / 15 : Datasets/Piano/Audio/Piano_12.wav\n",
      "processing the audio 13 / 15 : Datasets/Piano/Audio/Piano_13.wav\n",
      "processing the audio 14 / 15 : Datasets/Piano/Audio/Piano_14.wav\n",
      "processing the audio 15 / 15 : Datasets/Piano/Audio/Piano_15.wav\n"
     ]
    }
   ],
   "source": [
    "z_datasets = np.zeros(shape=(audio_num,1,1000,16)) # shape of z =(1, 1000, 16)\n",
    "\n",
    "i = 0\n",
    "for audio_path in audio_folder:\n",
    "    print('processing the audio',i+1,'/',audio_num,':',audio_path)\n",
    "    \n",
    "    x_all, sr = sf.read(audio_path) #data,samplerate\n",
    "    #print('shape of original signal:',np.shape(x_all),'\\n','original sample rate:',sr)\n",
    "    sig = x_all[:] # choose the first channel of the original audio\n",
    "\n",
    "    # resample (down sampling to 16kHz)\n",
    "    audio = librosa.resample(sig,sr,sample_rate)\n",
    "    #print('audio shape:',np.shape(audio))\n",
    "    audio = audio[np.newaxis,:]\n",
    "\n",
    "    #extracting f0 with CREPE\n",
    "    ddsp.spectral_ops.reset_crepe()\n",
    "    f0_crepe, f0_confidence = ddsp.spectral_ops.compute_f0(audio[0], \n",
    "                                                           sample_rate= sample_rate,\n",
    "                                                           frame_rate=31.25,\n",
    "                                                           viterbi=False)\n",
    "    #extracting loudness \n",
    "    loudness =ddsp.spectral_ops.compute_loudness(audio[0],\n",
    "                         sample_rate= sample_rate,\n",
    "                         frame_rate=250,\n",
    "                         n_fft=2048,\n",
    "                         ref_db=20.7,\n",
    "                         use_tf=False)\n",
    "\n",
    "    # audio_features dictionary\n",
    "    audio_features_key = ['audio','f0_hz','f0_confidence','loundness_db']\n",
    "    audio_features = dict([(k,[]) for k in audio_features_key])\n",
    "    audio_features['audio'] = audio\n",
    "    audio_features['f0_hz'] = f0_crepe\n",
    "    audio_features['f0_confidence'] = f0_confidence\n",
    "    audio_features['loudness_db'] = loudness\n",
    "    \n",
    "    # Trim all input vectors to correct lengths\n",
    "    time_steps = int(audio.shape[1] / hop_size)\n",
    "    n_samples = time_steps * hop_size \n",
    "    for key in ['f0_hz', 'f0_confidence', 'loudness_db']:\n",
    "        audio_features[key] = audio_features[key][:time_steps]\n",
    "    audio_features['audio'] = audio_features['audio'][:n_samples]\n",
    "    \n",
    "    # get z feature of the audio\n",
    "    outputs = model(audio_features, training=False) # Run the forward pass, add losses, and create a dictionary of outputs.\n",
    "    z_feature = outputs['z']\n",
    "    print('shape of z feature',np.shape(z_feature))\n",
    "    z_datasets[i] = z_feature\n",
    "    \n",
    "    i += 1\n",
    "    \n",
    "print('finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "auburn-arkansas",
   "metadata": {},
   "source": [
    "### Save z datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "appreciated-labor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./z_datasets/z_piano_ae.npy\n",
      "(15, 1, 1000, 16) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "save_path = './z_datasets/z_' + model_name + '.npy'\n",
    "print(save_path)\n",
    "np.save(save_path, z_datasets)\n",
    "print(np.shape(z_datasets),'\\n')\n",
    "#print(z_datasets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}