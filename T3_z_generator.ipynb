{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "broadband-cornell",
   "metadata": {},
   "source": [
    "# Generate z datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formal-theory",
   "metadata": {},
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "floating-excerpt",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Argument whitelist is deprecated. Please use allowlist.\n"
     ]
    }
   ],
   "source": [
    "# Ignore a bunch of deprecation warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import tensorflow.compat.v2 as tf\n",
    "import tensorflow_datasets as tfds \n",
    "import ddsp\n",
    "import utils\n",
    "import os\n",
    "import gin\n",
    "import pickle\n",
    "import matplotlib\n",
    "import IPython.display as ipd\n",
    "import numpy as np\n",
    "import soundfile as sf \n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "import librosa\n",
    "import librosa.display\n",
    " \n",
    "%matplotlib inline\n",
    "sample_rate = 16000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honey-requirement",
   "metadata": {},
   "source": [
    "## Setting the path of audios and model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improved-trinity",
   "metadata": {},
   "source": [
    "### Model path with z encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "artistic-administrator",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model folder direction \n",
    "model_dir = 'Pretrained_Models_for_T2/piano_ae'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satisfactory-disaster",
   "metadata": {},
   "source": [
    "### Audio directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "spread-poison",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Datasets/Piano/Audio/Piano_01.wav', 'Datasets/Piano/Audio/Piano_02.wav', 'Datasets/Piano/Audio/Piano_03.wav', 'Datasets/Piano/Audio/Piano_04.wav', 'Datasets/Piano/Audio/Piano_05.wav', 'Datasets/Piano/Audio/Piano_06.wav', 'Datasets/Piano/Audio/Piano_07.wav', 'Datasets/Piano/Audio/Piano_08.wav', 'Datasets/Piano/Audio/Piano_09.wav', 'Datasets/Piano/Audio/Piano_10.wav', 'Datasets/Piano/Audio/Piano_11.wav', 'Datasets/Piano/Audio/Piano_12.wav', 'Datasets/Piano/Audio/Piano_13.wav', 'Datasets/Piano/Audio/Piano_14.wav', 'Datasets/Piano/Audio/Piano_15.wav']\n",
      "\n",
      " number of audios: 15\n"
     ]
    }
   ],
   "source": [
    "# audio directory path\n",
    "audio_dir = 'Datasets/Piano/Audio'\n",
    "audio_folder = []\n",
    "\n",
    "files = os.listdir(audio_dir)\n",
    "for tmp in files:\n",
    "    if os.path.splitext(tmp)[1] == '.wav' or os.path.splitext(tmp)[1] == '.mp3': \n",
    "        audio_folder.append(os.path.join(audio_dir,tmp))\n",
    "\n",
    "audio_folder.sort()\n",
    "print(audio_folder)\n",
    "audio_num = len(audio_folder)\n",
    "print('\\n number of audios:', audio_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accomplished-prospect",
   "metadata": {},
   "source": [
    "## Load the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "binary-gibraltar",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loading(audio,audio_features,model_dir,training=False):\n",
    "\n",
    "    model_name = os.path.basename(model_dir)\n",
    "\n",
    "    # dataset_statistics.pkl in .model folder\n",
    "    dataset_stats_file = os.path.join('\\n',model_dir, 'dataset_statistics.pkl')\n",
    "    \n",
    "    #print(f'Loading dataset statistics from {dataset_stats_file}')\n",
    "    try:\n",
    "      if tf.io.gfile.exists(dataset_stats_file):\n",
    "        with tf.io.gfile.GFile(dataset_stats_file, 'rb') as f:\n",
    "          DATASET_STATS = pickle.load(f)\n",
    "    except Exception as err:\n",
    "      print('Loading dataset statistics from pickle failed: {}.'.format(err))\n",
    "\n",
    "    # operative_config-0.gin in model folder\n",
    "    gin_file = os.path.join(model_dir, 'operative_config-0.gin')\n",
    "    \n",
    "    # Parse gin config,\n",
    "    with gin.unlock_config():\n",
    "      gin.parse_config_file(gin_file, skip_unknown=True)\n",
    "\n",
    "    # Assumes only one checkpoint in the folder, 'ckpt-[iter]`.\n",
    "    ckpt_files = [f for f in tf.io.gfile.listdir(model_dir) if 'ckpt' in f]\n",
    "    ckpt_name = ckpt_files[0].split('.')[0]\n",
    "    ckpt = os.path.join(model_dir, ckpt_name)\n",
    "\n",
    "    # Ensure dimensions and sampling rates are equal\n",
    "    time_steps_train = gin.query_parameter('F0LoudnessPreprocessor.time_steps')\n",
    "    n_samples_train = gin.query_parameter('Harmonic.n_samples')\n",
    "    hop_size = int(n_samples_train / time_steps_train)\n",
    "\n",
    "    time_steps = int(audio.shape[1] / hop_size)\n",
    "    n_samples = time_steps * hop_size\n",
    "\n",
    "    # print(\"===Trained model===\")\n",
    "    # print(\"Time Steps\", time_steps_train)\n",
    "    # print(\"Samples\", n_samples_train)\n",
    "    # print(\"Hop Size\", hop_size)\n",
    "    # print(\"\\n===Resynthesis===\")\n",
    "    # print(\"Time Steps\", time_steps)\n",
    "    # print(\"Samples\", n_samples)\n",
    "    # print('')\n",
    "\n",
    "    gin_params = [\n",
    "        'Harmonic.n_samples = {}'.format(n_samples),\n",
    "        'FilteredNoise.n_samples = {}'.format(n_samples),\n",
    "        'F0LoudnessPreprocessor.time_steps = {}'.format(time_steps),\n",
    "        'oscillator_bank.use_angular_cumsum = True',  # Avoids cumsum accumulation errors.\n",
    "    ]\n",
    "\n",
    "    with gin.unlock_config():\n",
    "      gin.parse_config(gin_params)\n",
    "\n",
    "\n",
    "    # Trim all input vectors to correct lengths \n",
    "    for key in ['f0_hz', 'f0_confidence', 'loudness_db']:\n",
    "      audio_features[key] = audio_features[key][:time_steps]\n",
    "    audio_features['audio'] = audio_features['audio'][:n_samples]\n",
    "\n",
    "    # Set up the model just to predict audio given new conditioning\n",
    "    model = ddsp.training.models.Autoencoder()\n",
    "    model.restore(ckpt)\n",
    "\n",
    "    # Resynthesize audio.\n",
    "    outputs = model(audio_features, training) # Run the forward pass, add losses, and create a dictionary of outputs.\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amazing-instrumentation",
   "metadata": {},
   "source": [
    "## Generate z datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raised-blood",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing the audio 1 / 15 : Datasets/Piano/Audio/Piano_01.wav\n",
      "shape of z feature (1, 15000, 16)\n",
      "processing the audio 2 / 15 : Datasets/Piano/Audio/Piano_02.wav\n",
      "shape of z feature (1, 15000, 16)\n",
      "processing the audio 3 / 15 : Datasets/Piano/Audio/Piano_03.wav\n",
      "shape of z feature (1, 15000, 16)\n",
      "processing the audio 4 / 15 : Datasets/Piano/Audio/Piano_04.wav\n",
      "shape of z feature (1, 15000, 16)\n",
      "processing the audio 5 / 15 : Datasets/Piano/Audio/Piano_05.wav\n",
      "shape of z feature (1, 15000, 16)\n",
      "processing the audio 6 / 15 : Datasets/Piano/Audio/Piano_06.wav\n"
     ]
    }
   ],
   "source": [
    "z_datasets = np.zeros(shape=(audio_num,1,250*60,16)) # shape of z =(1, frame_rate*length_audio, 16)\n",
    "\n",
    "i = 0\n",
    "for audio_path in audio_folder:\n",
    "    print('processing the audio',i+1,'/',audio_num,':',audio_path)\n",
    "    \n",
    "    x_all, sr = sf.read(audio_path) #data,samplerate\n",
    "    #print('shape of original signal:',np.shape(x_all),'\\n','original sample rate:',sr)\n",
    "    sig = x_all[:] # choose the first channel of the original audio\n",
    "\n",
    "    # resample (down sampling to 16kHz) and take the 10-20 seconds\n",
    "    sig_re = librosa.resample(sig,sr,sample_rate)\n",
    "    audio = sig_re#[10*sample_rate:30*sample_rate]\n",
    "    #print('audio shape:',np.shape(audio))\n",
    "    audio = audio[np.newaxis,:]\n",
    "\n",
    "    #extracting f0 with CREPE\n",
    "    ddsp.spectral_ops.reset_crepe()\n",
    "    f0_crepe, f0_confidence = ddsp.spectral_ops.compute_f0(audio[0], \n",
    "                                                           sample_rate= sample_rate,\n",
    "                                                           frame_rate=250,\n",
    "                                                           viterbi=False)\n",
    "    #extracting loudness \n",
    "    loudness =ddsp.spectral_ops.compute_loudness(audio[0],\n",
    "                         sample_rate= sample_rate,\n",
    "                         frame_rate=250,\n",
    "                         n_fft=2048,\n",
    "                         ref_db=20.7,\n",
    "                         use_tf=False)\n",
    "\n",
    "    # audio_features dictionary\n",
    "    audio_features_key = ['audio','f0_hz','f0_confidence','loudness_db']\n",
    "    audio_features = dict([(k,[]) for k in audio_features_key])\n",
    "    audio_features['audio'] = audio\n",
    "    audio_features['f0_hz'] = f0_crepe\n",
    "    audio_features['f0_confidence'] = f0_confidence\n",
    "    audio_features['loudness_db'] = loudness\n",
    "    \n",
    "    \n",
    "    # get z feature of the audio\n",
    "    outputs = model_loading(audio, audio_features, model_dir, training = False) # Run the forward pass, add losses, and create a dictionary of outputs.\n",
    "    z_feature = outputs['z']\n",
    "    print('shape of z feature',np.shape(z_feature))\n",
    "    z_datasets[i] = z_feature\n",
    "    \n",
    "    i += 1\n",
    "    \n",
    "print('finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specified-overall",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outputs.keys())\n",
    "\n",
    "z_feature = outputs['z']\n",
    "print(np.shape(z_feature),'\\n',z_feature)\n",
    "\n",
    "print(np.shape(audio_features['f0_hz']), np.shape(outputs_z['f0_hz']))\n",
    "\n",
    "print(np.shape(audio_features['loudness_db']), np.shape(outputs_z['loudness_db']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "auburn-arkansas",
   "metadata": {},
   "source": [
    "### Save z datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appreciated-labor",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = './z_datasets/z_' + model_name + '.npy'\n",
    "print(save_path)\n",
    "np.save(save_path, z_datasets)\n",
    "print(np.shape(z_datasets),'\\n')\n",
    "#print(z_datasets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
